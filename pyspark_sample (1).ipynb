{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Given billions of numbers (assume each \n",
    "record has a single number), find minimum, \n",
    "maximum, average for all numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "we=[2,4,5,0,9,90,80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(Partitions):\n",
    "    first_time=False\n",
    "    min2=0\n",
    "    max2=0\n",
    "    for val in Partitions:\n",
    "        if first_time==False:\n",
    "            min2=val\n",
    "            max2=val\n",
    "            first_time=True\n",
    "        else:\n",
    "            min2=min(min2,val)\n",
    "            max2=max(max2,val)\n",
    "    \n",
    "    return [(min2, max2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd200=rdd.mapPartitions(find_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd201=rdd.reduce(lambda x,y: x+y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd202=rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd203=rdd201/rdd202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.142857142857142"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max=rdd200.reduce(lambda x, y: (min(x[0], y[0]), max(x[1], y[1]), rdd203))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 90, 27.142857142857142)"
      ]
     },
     "execution_count": 1028,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Given billions of numbers (assume each \n",
    "record has a single number), find count of\n",
    "zeros, positives, and negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums=[2,4,5,0,9,90,80,0,-9,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting(Partitions):\n",
    "    ze=0\n",
    "    nega=0\n",
    "    posi=0\n",
    "    for val in Partitions: \n",
    "        if val<0:\n",
    "            nega+=1\n",
    "        elif (val ==0):\n",
    "            ze+=1\n",
    "        else:\n",
    "            posi+=1\n",
    "    return [(ze, posi, nega)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=spark.sparkContext.parallelize(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd2.mapPartitions(counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2, 0), (1, 1, 0), (0, 2, 0), (1, 1, 2)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4= rdd3.reduce(lambda x,y: ((x[0]+y[0]), (x[1]+y[1]), (x[2]+y[2]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Consider the following input record format:\n",
    "<studentID , single-grade-in-range-of-0-to-100>\n",
    "\n",
    "The goal is to find minimum and maximum grades \n",
    "for all students. Write a PySpark program to \n",
    "accomplish this task. Your output will be \n",
    "\n",
    "<studentID, minimum-grade, maximum-grade>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=(1, 10), (2,80), (3,55), (1,75),(2, 90), (1, 20), (3, 50), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5=spark.sparkContext.parallelize(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 80), (3, 55), (1, 75), (2, 90), (1, 20), (3, 50)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd8=rdd5.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd9=rdd8.mapValues(lambda x: (min(x), max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (10, 75)), (2, (80, 90)), (3, (50, 55))]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Consider the following input record format:\n",
    "<movie-name, rating-in-range-of-1-to-5>\n",
    "\n",
    "The goal is to find the number of raters per movie\n",
    "Write a PySpark program to accomplish this task. \n",
    "Your output will be \n",
    "<movie-name, number-of-raters>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=('a', 4), ('b', 4), ('c', 3), ('a',1),('a', 2),('b', 2),('a',1),('a', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd10=spark.sparkContext.parallelize(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd11=rdd10.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=rdd11.mapValues(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('c', 1), ('a', 5)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Assume the following input\n",
    "<gene-ID , reference , gene-value>\n",
    "\n",
    "where reference can be:\n",
    "\"r1\": as normal\n",
    "\"r2\": as cancer\n",
    "\"r3\": as unknown\n",
    "\n",
    "The goal is to write a PySpark program to \n",
    "keep only normal genes and finally count \n",
    "them for all genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = (\"g1\", \"r1\", 3), (\"g2\", \"r1\", 4), (\"g3\", \"r2\", 3), (\"g4\", \"r2\", 1), (\"g5\", \"r3\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd12=spark.sparkContext.parallelize(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd13=rdd12.filter(lambda x: x[1]==\"r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g1', 'r1', 3), ('g2', 'r1', 4)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd13.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd13.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Let a bigram be defined as a sequence \n",
    "of two consecutive words. For example for \n",
    "the following input: \"w1,w2,w3,w4\",\n",
    "we can construct the following bigrams:\n",
    "\n",
    "w1, w2\n",
    "w2, w3\n",
    "w3, w4\n",
    "\n",
    "Let your input be a text file (x.dat), where \n",
    "each record has the following format (a record \n",
    "may have any number of words):\n",
    "<word1,word2,word3>...\n",
    "\n",
    "Write a PySpark program to find frequency \n",
    "of all unique bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path= '/Users/supriyatiwari/documents/PySpark_docs/bigram.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd14= spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['words me you her she is mad but me we', 'mad but me we', 'words me you']"
      ]
     },
     "execution_count": 1031,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd14.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd15=rdd14.map(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['words', 'me', 'you', 'her', 'she', 'is', 'mad', 'but', 'me', 'we'],\n",
       " ['mad', 'but', 'me', 'we'],\n",
       " ['words', 'me', 'you']]"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd15.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd16=rdd15.flatMap(lambda x: [((x[i], x[i+1]), 1) for i in range (0,len(x)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('words', 'me'), 1),\n",
       " (('me', 'you'), 1),\n",
       " (('you', 'her'), 1),\n",
       " (('her', 'she'), 1),\n",
       " (('she', 'is'), 1),\n",
       " (('is', 'mad'), 1),\n",
       " (('mad', 'but'), 1),\n",
       " (('but', 'me'), 1),\n",
       " (('me', 'we'), 1),\n",
       " (('mad', 'but'), 1),\n",
       " (('but', 'me'), 1),\n",
       " (('me', 'we'), 1),\n",
       " (('words', 'me'), 1),\n",
       " (('me', 'you'), 1)]"
      ]
     },
     "execution_count": 1043,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd16.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd17 = rdd16.reduceByKey(lambda x,y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('words', 'me'), 2),\n",
       " (('me', 'you'), 2),\n",
       " (('you', 'her'), 1),\n",
       " (('her', 'she'), 1),\n",
       " (('she', 'is'), 1),\n",
       " (('is', 'mad'), 1),\n",
       " (('mad', 'but'), 2),\n",
       " (('but', 'me'), 2),\n",
       " (('me', 'we'), 2)]"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd17.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Provide solution in MapReduce and PySpark\n",
    "Given the following input, write a classic \n",
    "map() and reduce() functions to find the maximum \n",
    "and minimum of all given keys and values for all \n",
    "given records.  For the following input (listed \n",
    "below), the output (output of all reducers) will \n",
    "be:\n",
    "\n",
    "min  10\n",
    "max  90\n",
    "\n",
    "Input is given as a set of (K, V) pairs:\n",
    "\n",
    "90    20\n",
    "40    70\n",
    "10    40\n",
    "30    40\n",
    "40    90\n",
    "30    80\n",
    "20    30\n",
    "20    10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [(90, 20), (40, 70), (10,40), (30,40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=spark.sparkContext.parallelize(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_new = rdd1.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90, 20, 40, 70, 10, 40, 30, 40]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_new.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_min=rdd_new.reduce(lambda x, y: min(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_max=rdd_new.reduce(lambda x, y: max(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:10,max:90\n"
     ]
    }
   ],
   "source": [
    "print(\"min:\" + str(rdd_min) + \",\" + \"max:\" + str(rdd_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Given the following rdd in pyspark:\n",
    "\n",
    "data = ['k1', 'k2', 'k1', 'k2', \n",
    "            'k1', 'k2', 'k3', 'k2']\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "write a sequence of pyspark transformations and actions to \n",
    "find frequencies  of all keys in data. For this example, your \n",
    "solution should generate/output:\n",
    "('k1', 3)\n",
    "('k2', 4)\n",
    "\n",
    "If a frequency is less than 2, then drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = ['k1', 'k2', 'k1', 'k2', 'k1', 'k2', 'k3', 'k2'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd18 = sc.parallelize(data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd19=rdd18.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 1),\n",
       " ('k2', 1),\n",
       " ('k1', 1),\n",
       " ('k2', 1),\n",
       " ('k1', 1),\n",
       " ('k2', 1),\n",
       " ('k3', 1),\n",
       " ('k2', 1)]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd19.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd20=rdd19.reduceByKey(lambda x,y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 3), ('k2', 4), ('k3', 1)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd20.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_r=rdd20.filter(lambda x: x[1]>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 3), ('k2', 4)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_r.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. PySpark and MapReduce solutions.\n",
    "Given large set of documents, we want to use \n",
    "classic MapReduce to create an \"inverted index\" \n",
    "for all documents. For example, given the following \n",
    "input documents:\n",
    "\n",
    "Document1: fox jumped fast fox fast\n",
    "Document2: fox ran fox jumped fast\n",
    "Document3: hello hello hello fox\n",
    "...\n",
    "\n",
    "we want to generate the following \"inverted index\"\n",
    "\n",
    "fox →  (Document1: 1, 4)(Document2: 1, 3)(Document3: 4)\n",
    "jumped  → (Document1: 2)(Document2: 4)\n",
    "fast → (Document1: 3, 5)(Document2: 5)\n",
    "ran  → (Document2: 2)\n",
    "hello  → (Document3: 1, 2, 3)\n",
    "...\n",
    "\n",
    "The goal is to develop a classic MapReduce program \n",
    "for inverted index creation: generate a list of \n",
    "locations (word number in the document and identifier \n",
    "for the document) for each word occurrence. An \n",
    "identifier for each document is provided as the key \n",
    "to the map() function and value is a string of words \n",
    "(for example, \"hello hello hello fox\" will be the value \n",
    "for the \"Document3\").\n",
    "\n",
    "a. Write a map() function \n",
    "you must identify Key and Value for the map()\n",
    "\n",
    "b. Write a reduce() function (NOT a PySpark function): \n",
    "you must identify Key and Value for the reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='/Users/supriyatiwari/documents/PySpark_docs/inverted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Document1: fox jumped fast fox fast',\n",
       " 'Document2: fox ran fox jumped fast',\n",
       " 'Document3: hello hello hello fox']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(input_path)\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Document1', ' fox jumped fast fox fast'],\n",
       " ['Document2', ' fox ran fox jumped fast'],\n",
       " ['Document3', ' hello hello hello fox']]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda a: a.split(\":\"))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(text):\n",
    "    l = []\n",
    "    words = []\n",
    "    words = text[1].split(\" \")\n",
    "    count = 1\n",
    "    for word in words:\n",
    "        l.append((word, (text[0], count)))\n",
    "        count += 1\n",
    "    return (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('', ('Document1', 1)),\n",
       "  ('fox', ('Document1', 2)),\n",
       "  ('jumped', ('Document1', 3)),\n",
       "  ('fast', ('Document1', 4)),\n",
       "  ('fox', ('Document1', 5)),\n",
       "  ('fast', ('Document1', 6))],\n",
       " [('', ('Document2', 1)),\n",
       "  ('fox', ('Document2', 2)),\n",
       "  ('ran', ('Document2', 3)),\n",
       "  ('fox', ('Document2', 4)),\n",
       "  ('jumped', ('Document2', 5)),\n",
       "  ('fast', ('Document2', 6))],\n",
       " [('', ('Document3', 1)),\n",
       "  ('hello', ('Document3', 2)),\n",
       "  ('hello', ('Document3', 3)),\n",
       "  ('hello', ('Document3', 4)),\n",
       "  ('fox', ('Document3', 5))]]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda a: find_labels(a))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', [('Document1', 1), ('Document2', 1), ('Document3', 1)]),\n",
       " ('ran', [('Document2', 3)]),\n",
       " ('fox',\n",
       "  [('Document1', 2),\n",
       "   ('Document1', 5),\n",
       "   ('Document2', 2),\n",
       "   ('Document2', 4),\n",
       "   ('Document3', 5)]),\n",
       " ('jumped', [('Document1', 3), ('Document2', 5)]),\n",
       " ('fast', [('Document1', 4), ('Document1', 6), ('Document2', 6)]),\n",
       " ('hello', [('Document3', 2), ('Document3', 3), ('Document3', 4)])]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.flatMap(lambda x: x).groupByKey().mapValues(lambda iter: list(iter))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_labels(i):\n",
    "    d = {}\n",
    "    for item in i:\n",
    "        if item[0] in d:\n",
    "            d[item[0]].append(item[1])\n",
    "        else:\n",
    "            d[item[0]] = [item[1]]\n",
    "    return (d)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', {'Document1': [1], 'Document2': [1], 'Document3': [1]}),\n",
       " ('fast', {'Document1': [4, 6], 'Document2': [6]}),\n",
       " ('fox', {'Document1': [2, 5], 'Document2': [2, 4], 'Document3': [5]}),\n",
       " ('hello', {'Document3': [2, 3, 4]}),\n",
       " ('jumped', {'Document1': [3], 'Document2': [5]}),\n",
       " ('ran', {'Document2': [3]})]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invert = rdd3.mapValues(lambda i: map_labels(i)).sortByKey()\n",
    "invert.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Assume that all of the input is in a file \n",
    "called  \"/dir/movies.txt\" and each input record \n",
    "has the following format:\n",
    "\n",
    "<userID, movieID, rating-in-range-of-1-to-5>\n",
    "\n",
    "Sample input:\n",
    "\n",
    "user1,movie1,3\n",
    "user1,movie1,1\n",
    "user1,movie2,5\n",
    "user2,movie1,4\n",
    "...\n",
    "...\n",
    "... \n",
    "\n",
    "Note that a user may rate the same movie any \n",
    "number of times. The goal is to find the number \n",
    "of raters per movie.  Write a PySpark program \n",
    "(as a set of trasformations and actions) to \n",
    "accomplish this task. Your output will be\n",
    "\n",
    "\"movieID number-of-raters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='/Users/supriyatiwari/documents/PySpark_docs/movies.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd21 = spark.sparkContext.textFile(input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user1,movie1,3',\n",
       " 'user1,movie1,1',\n",
       " 'user1,movie2,5',\n",
       " 'user2,movie1,4',\n",
       " 'user1,movie1,3',\n",
       " 'user1,movie1,1',\n",
       " 'user1,movie2,5',\n",
       " 'user2,movie1,4']"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd21.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['user1', 'movie1', '3'],\n",
       " ['user1', 'movie1', '1'],\n",
       " ['user1', 'movie2', '5'],\n",
       " ['user2', 'movie1', '4'],\n",
       " ['user1', 'movie1', '3'],\n",
       " ['user1', 'movie1', '1'],\n",
       " ['user1', 'movie2', '5'],\n",
       " ['user2', 'movie1', '4']]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd22 = rdd21.map(lambda a: a.split(\",\"))\n",
    "rdd22.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd23 = rdd22.map(lambda a: (a[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie2', 1),\n",
       " ('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie2', 1),\n",
       " ('movie1', 1)]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd23.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd24=rdd23.reduceByKey(lambda x,y:(x+y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie1', 6), ('movie2', 2)]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd24.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Consider the following in PySpark:\n",
    "\n",
    "/data = [1, 1, 2, 3, 1, 2, 3, 3, 3]\n",
    "/rdd = sc.parallelize(data, 3)\n",
    "/rdd2 = rdd.map(lambda x: (x, 2))\n",
    " /groupedby = rdd2.groupByKey().collect()\n",
    " /reducedby = rdd2.reduceByKey(lambda x, y: x * y).collect()\n",
    "\n",
    "a. Show the content of \"groupedby\" in detail\n",
    "show your work...\n",
    "\n",
    "b. Show the content of \"reducedby\" in detail\n",
    "show your work...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [1, 1, 2, 3, 1, 2, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd25 = sc.parallelize(data2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 1, 2, 3, 3, 3]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd25.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd26 = rdd25.map(lambda x: (x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 2), (2, 2), (3, 2), (1, 2), (2, 2), (3, 2), (3, 2), (3, 2)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd26.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedby = rdd26.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Let genes.txt be a huge text file, where every \n",
    "record has the following format (reference can be \n",
    "in {1, 2, 3} where 1 denotes cancer, 2 denotes \n",
    "healthy, and 3 is undefined):\n",
    "\n",
    "<geneID><,><reference><,><geneValue>\n",
    "\n",
    "For example, a sample input might be:\n",
    "\n",
    "g1,1, 2.3\n",
    "g1,2, 1.5\n",
    "g1,3,2.5\n",
    "g1,1,4.1\n",
    "g2,1,1.3\n",
    "g2,2,1.8\n",
    "g2,3,3.5\n",
    "g2,1,4.3\n",
    "g2,1,2.9\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "a. Write a PySpark command/tranformation to\n",
    "convert genes.txt file into an RDD<String> \n",
    "and then output the number of elements in \n",
    "that RDD\n",
    "\n",
    "b. Let rdd1 represents the genes.txt file in \n",
    "Spark.  Use rdd1 and write a PySpark command/\n",
    "tranformation to generate the following output \n",
    "per geneID\n",
    "\n",
    "<geneID> <C> <S>\n",
    "\n",
    "where C is the number of cancer genes (for geneID) \n",
    "and S is the sum of values for the cancer gene.\n",
    "\n",
    "c. Let rdd1 represents the genes.txt file in Spark.\n",
    "Use rdd1 and write a PySpark filter to remove all \n",
    "undefined genes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path= '/Users/supriyatiwari/documents/PySpark_docs/genes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd27 = spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g1,1,2.3',\n",
       " 'g1,2,1.5',\n",
       " 'g1,3,2.5',\n",
       " 'g1,1,4.1',\n",
       " 'g2,1,1.3',\n",
       " 'g2,2,1.8',\n",
       " 'g2,3,3.5',\n",
       " 'g2,1,4.3',\n",
       " 'g2,1,2.9']"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd27.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd28=rdd27.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['g1', '1', '2.3'],\n",
       " ['g1', '2', '1.5'],\n",
       " ['g1', '3', '2.5'],\n",
       " ['g1', '1', '4.1'],\n",
       " ['g2', '1', '1.3'],\n",
       " ['g2', '2', '1.8'],\n",
       " ['g2', '3', '3.5'],\n",
       " ['g2', '1', '4.3'],\n",
       " ['g2', '1', '2.9']]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd28.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd29=rdd28.map(lambda x: (x[0], float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g1', 2.3),\n",
       " ('g1', 1.5),\n",
       " ('g1', 2.5),\n",
       " ('g1', 4.1),\n",
       " ('g2', 1.3),\n",
       " ('g2', 1.8),\n",
       " ('g2', 3.5),\n",
       " ('g2', 4.3),\n",
       " ('g2', 2.9)]"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd29.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd30=rdd29.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g1', <pyspark.resultiterable.ResultIterable at 0x7fad6d06db50>),\n",
       " ('g2', <pyspark.resultiterable.ResultIterable at 0x7fad6d06d550>)]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd30.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd30=rdd29.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g1', <pyspark.resultiterable.ResultIterable at 0x7fad6d08edd0>),\n",
       " ('g2', <pyspark.resultiterable.ResultIterable at 0x7fad6d08ec90>)]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd30.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd31=rdd30.mapValues(lambda x: ( len(x), sum(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g1', (4, 10.399999999999999)), ('g2', (5, 13.799999999999999))]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd31.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd32=rdd27.filter(lambda x: x[0] !=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g1,1,2.3',\n",
       " 'g1,2,1.5',\n",
       " 'g1,3,2.5',\n",
       " 'g1,1,4.1',\n",
       " 'g2,1,1.3',\n",
       " 'g2,2,1.8',\n",
       " 'g2,3,3.5',\n",
       " 'g2,1,4.3',\n",
       " 'g2,1,2.9']"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd32.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Assume we have 100 billion numbers saved in a \n",
    "file called big.txt (one number per record) and \n",
    "the goal is to find the number of zeros, positives, \n",
    "and negatives for all of these numbers. Write a \n",
    "spark/pyspark program to accomplish this task. Your \n",
    "client has asked you to write an efficient program \n",
    "for this otherwise he will not pay any money for \n",
    "your software!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path= '/Users/supriyatiwari/documents/PySpark_docs/big.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd33 = spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '4', '6', '-2', '0', '90', '20', '30', '0', '0', '-9']"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd33.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd34=rdd33.map(lambda x: (int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd34=rdd33.map(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2'],\n",
       " ['4'],\n",
       " ['6'],\n",
       " ['-2'],\n",
       " ['0'],\n",
       " ['90'],\n",
       " ['20'],\n",
       " ['30'],\n",
       " ['0'],\n",
       " ['0'],\n",
       " ['-9']]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd34.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd35=rdd34.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, -2, 0, 90, 20, 30, 0, 0, -9]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd35.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nums(Partitions):\n",
    "    ze=0\n",
    "    pos=0\n",
    "    nega=0\n",
    "    for val in Partitions: \n",
    "        if (val == 0):\n",
    "            ze+=1\n",
    "        elif (val > 0):\n",
    "            pos+=1\n",
    "        else:\n",
    "            nega+=1\n",
    "    return [(ze, pos, nega)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd36=rdd35.mapPartitions(find_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4, 1), (2, 2, 1)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd36.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd37=rdd36.reduce(lambda x,y: ((x[0]+y[0]), (x[1]+y[1]), (x[2]+y[2]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6, 2)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Assume that all of the input is in a file \n",
    "called  \"movies.txt\" and each input record has the \n",
    "following format:\n",
    "\n",
    "<userID,movieID,rating-in-range-of-1-to-5>\n",
    "\n",
    "Sample input:\n",
    "\n",
    "user1,movie1,3\n",
    "user1,movie1,1\n",
    "user1,movie2,5\n",
    "user2,movie1,4\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "Note that a user may rate the same movie any \n",
    "number of times.\n",
    "\n",
    "a. The goal is to find the number of raters \n",
    "per movie.  Write a complete PySpark program \n",
    "(as a set of trasformations and actions) to \n",
    "accomplish this task. Your output will be\n",
    "\n",
    "(movieID number-of-raters)\n",
    "\n",
    "b. The goal is to find the number of unique \n",
    "movies rated by each user. Write a complete \n",
    "PySpark program (as a set of trasformations \n",
    "and actions) to accomplish this task. Your \n",
    "output will be\n",
    "\n",
    "(userID number-of-unique-movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='/Users/supriyatiwari/documents/PySpark_docs/movies.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd38 = spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user1,movie1,3',\n",
       " 'user1,movie1,1',\n",
       " 'user1,movie2,5',\n",
       " 'user2,movie1,4',\n",
       " 'user1,movie1,3',\n",
       " 'user1,movie1,1',\n",
       " 'user1,movie2,5',\n",
       " 'user2,movie1,4']"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd38.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd39=rdd38.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['user1', 'movie1', '3'],\n",
       " ['user1', 'movie1', '1'],\n",
       " ['user1', 'movie2', '5'],\n",
       " ['user2', 'movie1', '4'],\n",
       " ['user1', 'movie1', '3'],\n",
       " ['user1', 'movie1', '1'],\n",
       " ['user1', 'movie2', '5'],\n",
       " ['user2', 'movie1', '4']]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd39.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd40=rdd39.map(lambda x: ((x[1]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie2', 1),\n",
       " ('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie2', 1),\n",
       " ('movie1', 1)]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd40.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd41=rdd40.reduceByKey(lambda x,y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie1', 6), ('movie2', 2)]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd41.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd42=rdd39.map(lambda x: ((x[0]), (x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', 'movie1'),\n",
       " ('user1', 'movie1'),\n",
       " ('user1', 'movie2'),\n",
       " ('user2', 'movie1'),\n",
       " ('user1', 'movie1'),\n",
       " ('user1', 'movie1'),\n",
       " ('user1', 'movie2'),\n",
       " ('user2', 'movie1')]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd42.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd43=rdd42.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', <pyspark.resultiterable.ResultIterable at 0x7fad6e322d10>),\n",
       " ('user2', <pyspark.resultiterable.ResultIterable at 0x7fad6e322e10>)]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd43.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq(values):\n",
    "    new=[]\n",
    "    for val in values:\n",
    "        if val not in new:\n",
    "            new.append(val)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd44=rdd43.mapValues(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', ['movie1', 'movie2']), ('user2', ['movie1'])]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd44.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd45=rdd44.mapValues(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', 2), ('user2', 1)]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd45.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. Consider the following in PySpark:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">>> data = [1, 1, 1, 2, 3, 1, 2, 3, 3, 3]\n",
    ">>> rdd = sc.parallelize(data, 3)\n",
    ">>> rdd2 = rdd.map(lambda x: (x, x))\n",
    ">>> groupedRDD = rdd2.groupByKey().mapValues(lambda x : sum(x)).collect()\n",
    "\n",
    "Show the content of groupedRDD in detail. show your work...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> (1,1),(1,1),(1,1).(2,2),(3,3),(1,1).(2,2),(3,3),(3,3),(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(1,[1,1,1,1]), (2,[2,2]),(3,[3,3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(1,4), (2,2),(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. Consider the following in PySpark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [1, 1, 1, 1, 2, 2, 3, 1, 2, 3, 3, 3]\n",
    ">>> rdd = sc.parallelize(data)\n",
    ">>> rdd2 = rdd.map(lambda x: (x+1, x))\n",
    ">>> reducedRDD = rdd2.reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "Show the content of reducedRDD in detail. show your work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(2,1),(2,1),(2,1),(2,1),(3,2),(3,2),(4,3),(2,1),(3,2),(4,3),(4,3),(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(2,5), (3,6),(4, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    ">>> data = [1, -1, 1, 1, 0, 0, 1, -2, \n",
    "            2, 3, 1, 2, -3, ...]\n",
    ">>> rdd = sc.parallelize(data)\n",
    "\n",
    "Write a series of spark transformations to split \n",
    "rdd into two RDDs: rddP will hold only non-negative \n",
    "numbers and rddN will hold only negative numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. Let genes.txt be a huge text file, where every "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record has the following format (reference can be \n",
    "in {1, 2, 3}  where 1 denotes cancer, 2 denotes \n",
    "healthy, and 3 is undefined):\n",
    "\n",
    "<geneID><,><reference><,><geneValue>\n",
    "\n",
    "For example, a sample input might be:\n",
    "\n",
    "g1,1,2.3\n",
    "g1,2,1.5\n",
    "g1,3,2.5\n",
    "g1,1,4.1\n",
    "g2,1,1.3\n",
    "g2,2,1.8\n",
    "g2,3,3.5\n",
    "g2,1,4.3\n",
    "g2,1,2.9\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "a. Write a PySpark command/tranformation to\n",
    "convert genes.txt file into an RDD<String> and \n",
    "then output the number of elements in that RDD \n",
    "(the final result will be in rdd1)\n",
    "\n",
    "b. Let rdd1 represents the genes.txt file \n",
    "in Spark ((as RDD<String>).  Use rdd1 and write \n",
    "a PySpark command/tranformation to generate the \n",
    "following output per geneID\n",
    "\n",
    "<geneID> <M> <N>\n",
    "\n",
    "where M is the number of cancer genes (for geneID) \n",
    "and N is the sum of values for the cancer genes.\n",
    "\n",
    "c. Find sum of the values for all genes.\n",
    "\n",
    "d.  Let rdd1 represents the genes.txt file \n",
    "(as RDD<String>) in Spark.  Use rdd1 and write \n",
    "a PySpark filter to keep only healthy genes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33. PySpark and MapReduce solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Given the following input \n",
    "(millions of (key, value) pairs), find average rating \n",
    "per movie [note that ratings of less than 2 must be \n",
    "ignored]. The same movie can be rated any number of times. \n",
    "\n",
    "Input is given as: Key is a movie-id and Value is \n",
    "a rating  between 1 and 5.\n",
    "\n",
    "Key  Value\n",
    "m1    1\n",
    "m1    3\n",
    "m1    1\n",
    "m1    5\n",
    "...  ...\n",
    "m2    5\n",
    "m2    4\n",
    "...  ...\n",
    "\n",
    "a. Write a map() function: must identify \n",
    "Key and Value and their data types for the map()\n",
    "\n",
    "b. Show output of all mappers for movies { m1, m2 }\n",
    "\n",
    "c. Write a reduce() function: must identify Key \n",
    "and Value for the reduce()\n",
    "\n",
    "d. Show all input to all reducers for movies { m1, m2 }\n",
    "\n",
    "e. Show output of all reducers for movies { m1, m2 }\n",
    "\n",
    "f. Find average per movie by using reduceByKey()\n",
    "\n",
    "g. Find average per movie by using groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "movi=[('m1',1),('m1',3), ('m1',1),('m1',4),('m2',5),('m2',4), ('m3',5), ('m4',3),('m4',4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd46=spark.sparkContext.parallelize(movi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m1', 1),\n",
       " ('m1', 3),\n",
       " ('m1', 1),\n",
       " ('m1', 4),\n",
       " ('m2', 5),\n",
       " ('m2', 4),\n",
       " ('m3', 5),\n",
       " ('m4', 3),\n",
       " ('m4', 4)]"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd46.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd47=rdd46.mapValues(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m1', (1, 1)),\n",
       " ('m1', (3, 1)),\n",
       " ('m1', (1, 1)),\n",
       " ('m1', (4, 1)),\n",
       " ('m2', (5, 1)),\n",
       " ('m2', (4, 1)),\n",
       " ('m3', (5, 1)),\n",
       " ('m4', (3, 1)),\n",
       " ('m4', (4, 1))]"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd47.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd48=rdd47.reduceByKey(lambda x,y: (x[0]+y[0] , x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m3', (5, 1)), ('m4', (7, 2)), ('m2', (9, 2)), ('m1', (9, 4))]"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd48.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd49=rdd48.mapValues(lambda x: (x[0]/x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m3', 5.0), ('m4', 3.5), ('m2', 4.5), ('m1', 2.25)]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd49.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd50=rdd46.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd51=rdd50.mapValues(lambda x: (sum(x)/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m3', 5.0), ('m4', 3.5), ('m2', 4.5), ('m1', 2.25)]"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd51.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 34. Using MapReduce and PySpark, \n",
    "write a series of transformations and actions to \n",
    "eliminate all duplicate records from a given big \n",
    "file called bigfile.txt.  Your output will be all \n",
    "of unique records contained in bigfile.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path= \"/Users/supriyatiwari/documents/PySpark_docs/big.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd95=spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd96=rdd95.map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, -2, 0, 90, 20, 30, 0, 0, -9]"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd96.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd97=rdd96.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, -2, 0, 90, 20, 30, -9]"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd97.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 35. Assume the following input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "<Employee-ID,type>\n",
    "\n",
    "where type can be:\n",
    "\"fulltime\"\n",
    "\"parttime\"\n",
    "\"contractor\"\n",
    "\n",
    "The goal is to write a PySpark program to count  \n",
    "\"fulltime\" and \"parttime\" employees. Your output \n",
    "should be something like:\n",
    "\n",
    "fulltime: <number-of-fulltime-employees>\n",
    "parttime: <number-of-parttime-employees>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "type=(12, \"fulltime\"), (13, \"parttime\" ),(14,\"contractor\"), (10, \"parttime\"),(1, \"fulltime\"),(18, \"fulltime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd52=spark.sparkContext.parallelize(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd53=rdd52.map(lambda x: (x[1], (x[0], int(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fulltime', (12, 1)),\n",
       " ('parttime', (13, 1)),\n",
       " ('contractor', (14, 1)),\n",
       " ('parttime', (10, 1)),\n",
       " ('fulltime', (1, 1)),\n",
       " ('fulltime', (18, 1))]"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd53.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd54=rdd53.filter(lambda x: (x[0] != \"contractor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fulltime', (12, 1)),\n",
       " ('parttime', (13, 1)),\n",
       " ('parttime', (10, 1)),\n",
       " ('fulltime', (1, 1)),\n",
       " ('fulltime', (18, 1))]"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd54.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd55=rdd54.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd56=rdd55.mapValues(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parttime', 2), ('fulltime', 3)]"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd56.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 36. Given the following rdd in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = ['k1', 'k2', 'k1', 'k2', \n",
    "            'k1', 'k2', 'k3', 'k2', 'k4']\n",
    ">>> rdd = sc.parallelize(data)\n",
    "\n",
    "write a sequence of pyspark transformations \n",
    "and actions to find frequencies of all keys \n",
    "in data. Keep only the (key, frequency) \n",
    "pairs if the frequency is greater than one.\n",
    "\n",
    "For this example, your solution should generate/output:\n",
    "('k1', 3)\n",
    "('k2', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "ke=['k1', 'k2', 'k1', 'k2', 'k1', 'k2', 'k3', 'k2', 'k4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd57=spark.sparkContext.parallelize(ke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd58=rdd57.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 1),\n",
       " ('k2', 1),\n",
       " ('k1', 1),\n",
       " ('k2', 1),\n",
       " ('k1', 1),\n",
       " ('k2', 1),\n",
       " ('k3', 1),\n",
       " ('k2', 1),\n",
       " ('k4', 1)]"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd58.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd59= rdd58.reduceByKey(lambda x,y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 3), ('k2', 4), ('k4', 1), ('k3', 1)]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd59.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd60=rdd59.filter(lambda x: (x[1]>1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 3), ('k2', 4)]"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd60.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 37. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [1, 1, 1, 1, 2, 2, 3, 1, 2, 3, 3, 3]\n",
    ">>> rdd = sc.parallelize(data)\n",
    ">>> rdd2 = rdd.map(lambda x: (x+1, x-1))\n",
    ">>> myoutput = rdd2.reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "Show the content of myoutput in detail. show your work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(2,0),(2,0),(2,0),(2,0),(3,1),(3,1),(2,0),(3,1),(4,2),(4,2),(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(2,0),(3,3),(4,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40. Assume we have 100 billion numbers saved\n",
    "in a file called big.txt (one number per record) \n",
    "and the goal is to find the number of positives \n",
    "(numbers greater than zero) and negatives (numbers \n",
    "less than zero) for all of these numbers. Write a \n",
    "Spark/PySpark program to accomplish this task. Your \n",
    "client has asked you to write an efficient program \n",
    "for this otherwise he will not pay any money for \n",
    "your software!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is same as question 26!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42. Assume that all of the input is in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a file called  \"movies.txt\" and each \n",
    "input record has the following format:\n",
    "\n",
    "<userID><,><movieID><,><rating-in-range-of-1-to-5>\n",
    "\n",
    "Sample input:\n",
    "\n",
    "user1,movie1,3\n",
    "user1,movie1,1\n",
    "user1,movie2,5\n",
    "user2,movie1,4\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "Note that a user may rate the same movie any \n",
    "number of times. You may use the following \n",
    "functions in your transformations:\n",
    "\n",
    "getUser(\"userX,movieY,ratingN\")   returns \"userX\"\n",
    "getMovie(\"userX,movieY,ratingN\")  returns \"movieY\"\n",
    "getRating(\"userX,movieY,ratingN\") returns ratingN\n",
    "\n",
    "MUST use the provided functions.\n",
    "\n",
    "a. The goal is to find the number of raters \n",
    "per movie.  Write a complete PySpark program \n",
    "(as a set of trasformations and actions) to \n",
    "accomplish this task. Your output will be\n",
    "\n",
    "<movieID> <number-of-raters>\n",
    "\n",
    "b. The goal is to find the number of unique \n",
    "movies rated by each user. Write a complete \n",
    "PySpark program (as a set of trasformations \n",
    "and actions) to accomplish this task. \n",
    "Your output will be\n",
    "\n",
    "<userID> <number-of-unique-movies>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path= \"/Users/supriyatiwari/documents/PySpark_docs/movies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd61=spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user1,movie1,3',\n",
       " 'user1,movie1,1',\n",
       " 'user1,movie2,5',\n",
       " 'user2,movie1,4',\n",
       " 'user1,movie1,3',\n",
       " 'user1,movie1,1',\n",
       " 'user1,movie2,5',\n",
       " 'user2,movie1,4']"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd61.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd62=rdd61.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['user1', 'movie1', '3'],\n",
       " ['user1', 'movie1', '1'],\n",
       " ['user1', 'movie2', '5'],\n",
       " ['user2', 'movie1', '4'],\n",
       " ['user1', 'movie1', '3'],\n",
       " ['user1', 'movie1', '1'],\n",
       " ['user1', 'movie2', '5'],\n",
       " ['user2', 'movie1', '4']]"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd62.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd63=rdd62.map(lambda x: (x[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie2', 1),\n",
       " ('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie1', 1),\n",
       " ('movie2', 1),\n",
       " ('movie1', 1)]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd63.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd64=rdd63.reduceByKey(lambda x,y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie1', 6), ('movie2', 2)]"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd64.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd65=rdd61.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd66=rdd65.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', 'movie1'),\n",
       " ('user1', 'movie1'),\n",
       " ('user1', 'movie2'),\n",
       " ('user2', 'movie1'),\n",
       " ('user1', 'movie1'),\n",
       " ('user1', 'movie1'),\n",
       " ('user1', 'movie2'),\n",
       " ('user2', 'movie1')]"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd66.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd67=rdd66.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', <pyspark.resultiterable.ResultIterable at 0x7fad6e851150>),\n",
       " ('user2', <pyspark.resultiterable.ResultIterable at 0x7fad6e851110>)]"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd67.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_movies(movies):\n",
    "    new=[]\n",
    "    for val in movies:\n",
    "        if val not in new:\n",
    "            new.append(val)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd68=rdd67.mapValues(unique_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user1', ['movie1', 'movie2']), ('user2', ['movie1'])]"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd68.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [1, 1, 1, 2, 3, 1, 2, 3, 3, 3]\n",
    ">>> rdd = sc.parallelize(data)\n",
    ">>> rdd2 = rdd.map(lambda x: (x, x+2))\n",
    ">>> groupedRDD = rdd2.groupByKey()\n",
    "                 .mapValues(lambda x : sum(x))\n",
    "                 .collect()\n",
    "\n",
    "Show the content of groupedRDD in detail. show your work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(1,3),(1,3),(1,3),(2,4),(3,5),(1,3),(2,4),(3,5),(3,5),(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(1,[3,3,3,3]), (2,[4,4]),(3,[5,5,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<(1,12),(2,8),(3,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [1, 1, 1, 1, 2, 2, 3, 1, 2, 3, 3, 3]\n",
    ">>> rdd = sc.parallelize(data)\n",
    ">>> rdd2 = rdd.map(lambda x: (x+1, x))\n",
    ">>> reducedRDD = rdd2.reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "Show the content of reducedRDD in detail. show your work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [1, -1, 1, 1, 0, 0, 1, -2, 2, 3, 1, 2, -3, ...]\n",
    ">>> rdd = sc.parallelize(data)\n",
    "\n",
    "Write a series of spark transformations to split rdd into \n",
    "two RDDs: rddP will hold only non-negative numbers\n",
    "and rddN will hold only negative numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "data45 = [1, -1, 1, 1, 0, 0, 1, -2, 2, 3, 1, 2, -3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd110=spark.sparkContext.parallelize(data45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd111=rdd110.filter(lambda x: x >0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd112=rdd110.filter(lambda x: x <0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 3, 1, 2]"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd111.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2, -3]"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd112.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46. Assume we have about 100 billion  \n",
    "numbers saved in a file called big.txt \n",
    "(one number  per record) and the goal is \n",
    "to perform the following\n",
    "in order (MUST USE PySpark):\n",
    "\n",
    "a. create an RDD<Integer> as rdd\n",
    "b. count the exact number of numbers in rdd\n",
    "c. remove all negative numbers\n",
    "d. count all remaining numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='/Users/supriyatiwari/documents/PySpark_docs/big.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd69=spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '4', '6', '-2', '0', '90', '20', '30', '0', '0', '-9']"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd69.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd70=rdd69.map(lambda x: (int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, -2, 0, 90, 20, 30, 0, 0, -9]"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd70.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd70.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd71=rdd70.filter(lambda x: (x>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd71.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [0, 1, 0, 1, -1, 1, 0, 2, 3, 1, -2, -3, 3, 3]\n",
    ">>> rdd = sc.parallelize(data)\n",
    ">>> rdd2 = rdd.filter(lambda v: v > 0)\n",
    ">>> rdd3 = rdd2.map(lambda x: (x, x+2))\n",
    ">>> reducedRDD = rdd3.reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "Show the content of reducedRDD in detail. \n",
    "show your work...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< [1,1,1,2,3,1,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<[(1,3), (1,3), (1,3), (2,4),(3,5), (1,3), (3,5), (3,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<[(1,12), (2,4), (3,15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 49. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [(\"a\", 1), (\"a\", 1), (\"a\", 3), \n",
    "            (\"b\", 1), (\"b\", 1), (\"b\", 2), \n",
    "            ...]\n",
    ">>> rdd = sc.parallelize(data)\n",
    "\n",
    "Write a series of spark transformations to find \n",
    "the maximum value per key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "data49=[(\"a\", 1), (\"a\", 1), (\"a\", 3),(\"b\", 1), (\"b\", 1), (\"b\", 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd72=spark.sparkContext.parallelize(data49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd73=rdd72.reduceByKey(lambda x,y: (max(x,y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 3)]"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd73.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50. Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let data represent a set of records:\n",
    "\n",
    ">>> data = [\"abc\", \"abc\", \"xyz\", \"xyz\", \"xyz\", ...]\n",
    ">>> rdd = sc.parallelize(data)\n",
    "\n",
    "Write a series of spark transformations to eliminate\n",
    "all duplicate records. For this example, the output\n",
    "will be : [\"abc\", \"xyz\", ...]. NOTE that you can NOT\n",
    "use unique() and distinct() transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "data50 = [\"abc\", \"abc\", \"xyz\", \"xyz\", \"xyz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd74=spark.sparkContext.parallelize(data50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd75=rdd74.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 1), ('abc', 1), ('xyz', 1), ('xyz', 1), ('xyz', 1)]"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd75.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd76=rdd75.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xyz', <pyspark.resultiterable.ResultIterable at 0x7fad6e886b50>),\n",
       " ('abc', <pyspark.resultiterable.ResultIterable at 0x7fad6e886b10>)]"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd76.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd77=rdd76.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyz', 'abc']"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd77.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 53. Given the following input (filebig.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " , using Spark's mapPartitions() write \n",
    "an efficient transformation to find minimum and \n",
    "maximum of all given numbers. Note that every \n",
    "record (single line of input) may have thousands \n",
    "of numbers.\n",
    "\n",
    "Input is given as:\n",
    "\n",
    "10,4,50,40,30, ...\n",
    "10,60,50,20, ...\n",
    "20,20,30,40,50,2, ...\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 54. Given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    ">>> def myfunc(n):\n",
    "...     if n < 0:\n",
    "...             return [n, -n, -n]\n",
    "...     else:\n",
    "...             return []\n",
    "...\n",
    ">>>\n",
    ">>> data = [0, 1, 2, -3, -4]\n",
    ">>> rdd = spark.sparkContext.parallelize(data)\n",
    ">>> rdd.collect()\n",
    "[0, 1, 2, -3, -4]\n",
    ">>> rdd.count()\n",
    "5\n",
    ">>> rdd3 = rdd.flatMap(myfunc).flatMap(myfunc)\n",
    ">>> rdd3.collect()\n",
    "\n",
    "What is the output of this program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-3,3,3,-4,4,4),\n",
    "<<<(-3,3,3,-4,4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 55. classic MapReduce and PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Given the following (key, value) pairs \n",
    "(as input to map()):\n",
    "\n",
    "<key-as-string> <value-as-integer>\n",
    "\n",
    "Write a complete map()  and reduce() functions to \n",
    "find the median per key. Only, Output medians, which \n",
    "are greater than 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 56. Use PySpark to answer this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assume that all of the input is in a file called  \n",
    "\"movies.txt\" (with millions of records) and each \n",
    "input record has the following format:\n",
    "\n",
    "<MOVIE-ID><,><rating-in-range-of-1-to-5>\n",
    "\n",
    "Sample input:\n",
    "\n",
    "movie1,3\n",
    "movie1,1\n",
    "movie1,5\n",
    "movie2,5\n",
    "movie2,4\n",
    "movie2,3\n",
    "...\n",
    "\n",
    "Note that a user may rate the same movie any number of \n",
    "times.  You HAVE to use the following Python functions \n",
    "in your transformations (NOTE, you MUST NOT use the \n",
    "split() function at all).\n",
    "\n",
    "getMovie(\"movie,rating\")  returns \"movie\" as String\n",
    "getRating(\"movie,rating\") returns rating as Integer\n",
    "\n",
    "a. The goal is to find the number of raters \n",
    "per movie.  Write a complete PySpark program (as a set of \n",
    "PySpark trasformations and actions) to accomplish this task. \n",
    "Your output will be like:\n",
    "\n",
    "<MOVIE-ID> <number-of-raters>\n",
    "\n",
    "b. The goal is to find the average rating \n",
    "per movie. Write a complete PySpark program (as a set of \n",
    "trasformations and actions) to accomplish this task. Your \n",
    "output will be as:\n",
    "\n",
    "<MOVIE-ID> <average-rating-per-MOVIE-ID>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='/Users/supriyatiwari/documents/PySpark_docs/movie.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd78=spark.sparkContext.textFile(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie1,3', 'movie1,1', 'movie1,5', 'movie2,5', 'movie2,4', 'movie2,3']"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd78.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd78.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-753-a77666da0adb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-753-a77666da0adb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def getMovie(\"movie,rating\"):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def getMovie(values):\n",
    "    for val in values: \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 57.  Consider the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [0, 2, 2, -3, 1, -1, 3, -2, -4, 3]\n",
    ">>> rdd = spark.sparkContext.parallelize(data)\n",
    ">>> print(\"output-1: \", rdd.collect())\n",
    ">>> rdd2 = rdd.filter(lambda v: v > 0)\n",
    ">>> print(\"output-2: \", rdd2.collect())\n",
    ">>> rdd3 = rdd2.map(lambda x: (x, x+2))\n",
    ">>> print(\"output-3: \", rdd3.collect())\n",
    ">>> rdd4 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    ">>> print(\"output-4: \", rdd4.collect())\n",
    "\n",
    "Show the output in detail.  show your work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0, 2, 2, -3, 1, -1, 3, -2, -4, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-1:  [0, 2, 2, -3, 1, -1, 3, -2, -4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"output-1: \", rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.filter(lambda v: v > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-2:  [2, 2, 1, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"output-2: \", rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x, x+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-3:  [(2, 4), (2, 4), (1, 3), (3, 5), (3, 5)]\n"
     ]
    }
   ],
   "source": [
    "print(\"output-3: \", rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-4:  [(1, 3), (2, 8), (3, 10)]\n"
     ]
    }
   ],
   "source": [
    "print(\"output-4: \", rdd4.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 61. Consider the following in PySpark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def fun7(x):\n",
    ">>>     if (x == 1):\n",
    ">>>         return [x, 1]\n",
    ">>>     if (x > 0):\n",
    ">>>         return [x, x, -2]\n",
    ">>>     return []\n",
    ">>> #end-def\n",
    ">>> data = [1, 1, -1, -2, 2, 2, -4]\n",
    ">>> rdd = spark.sparkContext.parallelize(data, 3)\n",
    ">>> rdd2 = rdd.flatMap(fun7)\n",
    ">>> rdd2.collect()\n",
    ">>> pairs = rdd2.map(lambda x: (x, 3))\n",
    "                .groupByKey()\n",
    "                .mapValues(lambda x : sum(x))\n",
    "                .collect()\n",
    "\n",
    "Show the output. show your work...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun7(x):\n",
    "    if (x == 1):\n",
    "        return [x, 1]\n",
    "    if (x > 0):\n",
    "        return [x, x, -2]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 1, -1, -2, 2, 2, -4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(fun7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 2, -2, 2, 2, -2]"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = rdd2.map(lambda x: (x, 3)).groupByKey().mapValues(lambda x : sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 12), (-2, 6), (2, 12)]"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 62. Consider the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(key, value) pairs in PySpark:\n",
    "\n",
    ">>> data = [('A', 4), ('A', 8), \n",
    "            ('B', 5), ('B', 7), ...]\n",
    ">>> rdd = sc.parallelize(data)\n",
    "\n",
    "Write a set of Spark transformations to \n",
    "find the average (mean) value per key. \n",
    "You CAN NOT use groupByKey().\n",
    "Show your work in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "data62 = [('A', 4), ('A', 8), ('B', 5), ('B', 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd79=spark.sparkContext.parallelize(data62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 4), ('A', 8), ('B', 5), ('B', 7)]"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd79.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd80=rdd79.mapValues(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (4, 1)), ('A', (8, 1)), ('B', (5, 1)), ('B', (7, 1))]"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd80.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd81=rdd80.reduceByKey(lambda x,y: ((x[0] + y[0]), (x[1]+y[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (12, 2)), ('B', (12, 2))]"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd81.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = rdd81.mapValues(lambda x: (x[0]/x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 6.0), ('B', 6.0)]"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 64. Given the following rdd of pairs \n",
    "in PySpark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> data = [('k1', 5), ('k1', 6), ('k1', 7), \n",
    "            ('k2', 7), ('k2', 8), ...]\n",
    ">>> rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "write a sequence of pyspark transformations and actions \n",
    "to find unique list of keys: {'k1', 'k2', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "data64=[('k1', 5), ('k1', 6), ('k1', 7), ('k2', 7), ('k2', 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd82= spark.sparkContext.parallelize(data64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd83=rdd82.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', <pyspark.resultiterable.ResultIterable at 0x7fad6e914490>),\n",
       " ('k2', <pyspark.resultiterable.ResultIterable at 0x7fad6e914450>)]"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd83.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd84=rdd83.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k1', 'k2']"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd84.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 65. Given the following rdd of \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pairs in PySpark:\n",
    "\n",
    ">>> data = [('k1', 5), ('k1', 6), ('k1', 7), \n",
    "            ('k2', 7), ('k2', 8), ...]\n",
    ">>> rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "write a sequence of pyspark transformations to \n",
    "find the minimum value per key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "data65 = [('k1', 5), ('k1', 6), ('k1', 7), ('k2', 7), ('k2', 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd85=spark.sparkContext.parallelize(data65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd86= rdd85.reduceByKey(lambda x,y:(min(x,y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 5), ('k2', 7)]"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd86.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 66. Given the following rdd of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pairs in PySpark:\n",
    "\n",
    ">>> data = [('a', 2), ('b', 3), ('d', 2), \n",
    "            ('x', 3), ('y', 1), ...]\n",
    ">>> rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "write a sequence of pyspark transformations to \n",
    "generate the following output: MUST use flatMap():\n",
    "\n",
    "[\n",
    " 'a', 'a', \n",
    " 'b', 'b', 'b', \n",
    " 'd', 'd', \n",
    " 'x', 'x', 'x', \n",
    " 'y',\n",
    "  ...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "data66=[('a', 2), ('b', 3), ('d', 2), ('x', 3), ('y', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd87 = spark.sparkContext.parallelize(data66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd88=rdd87.map(lambda x: (x[0] * x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'bbb', 'dd', 'xxx', 'y']"
      ]
     },
     "execution_count": 1061,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd88.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddrep=rdd88.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b', 'b', 'b', 'd', 'd', 'x', 'x', 'x', 'y']"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddrep.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 67:  Consider the following RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ">>> input = [(\"k1\", \"v1\"), (\"k1\", \"v1\"), (\"k1\", \"v2\"), ...]\n",
    ">>> RDD1 = sc.parallelize(input)\n",
    "The goal is to write a set of Spark transformations to generate\n",
    "unique (K, V) pairs [combination of K and V must be unique]. You\n",
    "may NOT use Spark's distinct() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "input67 = [(\"k1\", \"v1\"), (\"k1\", \"v1\"), (\"k1\", \"v2\"), (\"k1\", \"v2\"), (\"k1\", \"v2\"), (\"k3\", \"v2\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd89=spark.sparkContext.parallelize(input67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd90=rdd89.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd91=rdd90.groupByKey()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd222=rdd91.map(lambda x: (x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k3', 'v2'), ('k1', 'v1'), ('k1', 'v2')]"
      ]
     },
     "execution_count": 1057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd222.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73. Consider the following input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1\n",
    "11\n",
    "-1\n",
    "2\n",
    "12\n",
    "3\n",
    "-4\n",
    "13\n",
    "4\n",
    "14\n",
    "...\n",
    "\n",
    "Suppose, we want to  count all positive odd and even numbers. \n",
    "Write an efficient PySpark program to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "input73=[1,11,-1,2,12,3,-4,13,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd92=spark.sparkContext.parallelize(input73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd93= rdd92.filter(lambda x: (x>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd94=rdd93.reduce(lambda x,y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Given RDD[(String, Integer)], find median per key.  \n",
    "Provide two solutions: using groupByKey() and reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2000=[('Abc',3),('Abc',1),('Abc',2),('Abc',3),('Bdc',4),('Bdc',4),('Bdc',2),('Bdc',1),('Bdc',45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2000=spark.sparkContext.parallelize(data2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = rdd2000.map(lambda x: ((x[0]), (int(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abc', 3),\n",
       " ('Abc', 1),\n",
       " ('Abc', 2),\n",
       " ('Abc', 3),\n",
       " ('Bdc', 4),\n",
       " ('Bdc', 4),\n",
       " ('Bdc', 2),\n",
       " ('Bdc', 1),\n",
       " ('Bdc', 45)]"
      ]
     },
     "execution_count": 1095,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(x):\n",
    "    array_list = x[1]\n",
    "    med = np.median(array_list)\n",
    "    return(x[0], med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abc', [3, 1, 2, 3]), ('Bdc', [4, 4, 2, 1, 45])]"
      ]
     },
     "execution_count": 1097,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_key = pairs.groupByKey().mapValues(lambda iter: list(iter)).sortByKey()\n",
    "array_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_med = array_key.map(lambda rec: find_median(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abc', 2.5), ('Bdc', 4.0)]"
      ]
     },
     "execution_count": 1099,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_med.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2000=[('Abc',3),('Abc',1),('Abc',2),('Abc',3),('Bdc',4),('Bdc',4),('Bdc',2),('Bdc',1),('Bdc',45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2000=spark.sparkContext.parallelize(data2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(x):\n",
    "    med = median(x)\n",
    "    return(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2001=rdd2000.groupByKey().mapValues(lambda x: (list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2002=rdd2001.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2003=rdd2002.mapValues(find_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abc', 2.5), ('Bdc', 4.0)]"
      ]
     },
     "execution_count": 1124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2003.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
