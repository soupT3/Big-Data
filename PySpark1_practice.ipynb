{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = [('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find average per key using reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd.mapValues(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (7, 1)), ('A', (9, 1)), ('A', (2, 1)), ('B', (4, 1)), ('B', (3, 1))]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "red=rdd2.reduceByKey(lambda x, y : (x[0]+y[0], x[1] + y[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (18, 3)), ('B', (7, 2))]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=red.mapValues(lambda x: x[0]/ x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 6.0), ('B', 3.5)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Find average per key using groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', <pyspark.resultiterable.ResultIterable at 0x7febf7fd3ad0>),\n",
       " ('B', <pyspark.resultiterable.ResultIterable at 0x7febf7fd3d50>)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2=rdd3.mapValues(lambda x: sum(x)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 6.0), ('B', 3.5)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Given RDD[(String, Integer)], find median per key.  \n",
    "Provide two solutions: using groupByKey() and reduceByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Using groupByKey() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs= rdd.map(lambda x: (x[0], int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(x):\n",
    "    array_list=x[1]\n",
    "    med=np.median(array_list)\n",
    "    return (x[0], med)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_key=pairs.groupByKey().mapValues(lambda iter: list(iter)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', [7, 9, 2]), ('B', [4, 3])]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_med=array_key.map(lambda rec: find_median(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_med.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Given RDD[(String, Integer)], find average of positive numbers per key.  \n",
    "##Provide two solutions: using groupByKey() and reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = [('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3), ('B', -4), ('A', -2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_new=spark.sparkContext.parallelize(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3), ('B', -4), ('A', -2)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_new.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_f=rdd_new.filter(lambda x: (x[1] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_f.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_m = rdd_f.mapValues(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (7, 1)), ('A', (9, 1)), ('A', (2, 1)), ('B', (4, 1)), ('B', (3, 1))]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_m.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_r=rdd_m.reduceByKey(lambda x,y:(x[0]+y[0], x[1]+ y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (18, 3)), ('B', (7, 2))]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_final = rdd_r.mapValues(lambda x: x[0]/x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 6.0), ('B', 3.5)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_final.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2=rdd_new.filter(lambda x: (x[1] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 9), ('A', 2), ('B', 4), ('B', 3)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=rdd_2.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_fin=rdd3.mapValues(lambda x: sum(x)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 6.0), ('B', 3.5)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_fin.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Given RDD[(String, Integer)], find (minimum, maximum) per key.  \n",
    "Provide two solutions: using groupByKey() and reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4=[('A', 2),('A', 4), ('A', 3), ('B', 2), ('B', 3) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd=spark.sparkContext.parallelize(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 2), ('A', 4), ('A', 3), ('B', 2), ('B', 3)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = rd.map(lambda x: (x[0], int(x[1]))).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 2), ('A', 4), ('A', 3), ('B', 2), ('B', 3)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_rdd = pairs.map(lambda x: (x[0], (int(x[1]), int(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (2, 2)), ('A', (4, 4)), ('A', (3, 3)), ('B', (2, 2)), ('B', (3, 3))]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = pairs_rdd.reduceByKey(lambda x,y:(min(x[0],y[0]), max(x[1],y[1]))).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (2, 4)), ('B', (2, 3))]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6. Find min and max of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=[2, 4, 6, -2, 0 , 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddu=spark.sparkContext.parallelize(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(Partition): \n",
    "    first_time=False\n",
    "    for v in Partition: \n",
    "        if (first_time==False):\n",
    "            min2=v\n",
    "            max2=v\n",
    "            first_time=True\n",
    "        else: \n",
    "            min2=min(min2, v)\n",
    "            max2=max(max2, v)\n",
    "    return [(min2, max2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max=rddu.mapPartitions(find_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_min_max = min_max.reduce(lambda x,y: (min(x[0], y[0]), max(x[1], y[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 90)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Find count, min and max of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=[2, 4, 6, -2, 0 , 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_n=spark.sparkContext.parallelize(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_count_min_max(Partition): \n",
    "    first_time=False\n",
    "    counter=0\n",
    "    for v in Partition: \n",
    "        if (first_time==False):\n",
    "            min2=v\n",
    "            max2=v\n",
    "            first_time=True\n",
    "            counter+=1\n",
    "        else: \n",
    "            min2=min(min2, v)\n",
    "            max2=max(max2, v)\n",
    "            counter+=1\n",
    "    return [(counter, min2, max2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cou_min_max=rdd_n.mapPartitions(find_count_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 2), (2, 4, 6), (1, -2, -2), (2, 0, 90)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cou_min_max.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = cou_min_max.reduce(lambda x, y : ((x[0] + y[0]), min(x[1], y[1]), max(x[2], y[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, -2, 90)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Find min, max, number of +ves, number of -ves of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=[2, 4, 6, -2, 0 , 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_ne=spark.sparkContext.parallelize(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_posi_min_max(Partition): \n",
    "    first_time=False\n",
    "    posi = 0\n",
    "    nega = 0\n",
    "    for v in Partition: \n",
    "        if (first_time==False):\n",
    "            min2=v\n",
    "            max2=v\n",
    "            first_time=True\n",
    "            if (v<0):\n",
    "                nega+=1\n",
    "            else:\n",
    "                posi+=1\n",
    "        else: \n",
    "            min2=min(min2, v)\n",
    "            max2=max(max2, v)\n",
    "            if (v<0):\n",
    "                nega+=1\n",
    "            else:\n",
    "                posi+=1\n",
    "    return [( min2, max2, posi, nega)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi_min_max=rdd_ne.mapPartitions(find_posi_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2, 1, 0), (4, 6, 2, 0), (-2, -2, 0, 1), (0, 90, 2, 0)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posi_min_max.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ans = posi_min_max.reduce(lambda x, y : ( min(x[0], y[0]), max(x[1], y[1]), (x[2] + y[2]), (x[3] + y[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 90, 5, 1)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Write a series of PySpark transformations to find (average age of kids, average-number-of-candies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<age, no of candies>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(2,4), (2,3), (3,4), (4,5), (4,1), (5,4), (5,10), (5,5), (6, 0), (6,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd9 = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 4),\n",
       " (2, 3),\n",
       " (3, 4),\n",
       " (4, 5),\n",
       " (4, 1),\n",
       " (5, 4),\n",
       " (5, 10),\n",
       " (5, 5),\n",
       " (6, 0),\n",
       " (6, 7)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd10 = rdd9.map(lambda x: (x[0], 1 , x[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1, 4, 1),\n",
       " (2, 1, 3, 1),\n",
       " (3, 1, 4, 1),\n",
       " (4, 1, 5, 1),\n",
       " (4, 1, 1, 1),\n",
       " (5, 1, 4, 1),\n",
       " (5, 1, 10, 1),\n",
       " (5, 1, 5, 1),\n",
       " (6, 1, 0, 1),\n",
       " (6, 1, 7, 1)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd10.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd11 = rdd10.reduce(lambda x,y: ((x[0] + y[0]), (x[1] + y[1]), (x[2] + y[2]), (x[3] + y[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 10, 43, 10)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd12 =(rdd11[0]/rdd11[1], rdd11[2]/rdd11[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, 4.3)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sum_elements(Partitions): \n",
    "    sum1=0\n",
    "    sum2=0\n",
    "    count=0\n",
    "    for val in Partitions:\n",
    "        sum1+=val[0]\n",
    "        sum2+=val[1]\n",
    "        count+=1\n",
    "    return[(sum1, sum2, count)]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 4),\n",
       " (2, 3),\n",
       " (3, 4),\n",
       " (4, 5),\n",
       " (4, 1),\n",
       " (5, 4),\n",
       " (5, 10),\n",
       " (5, 5),\n",
       " (6, 0),\n",
       " (6, 7)]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd13=rdd9.mapPartitions(find_sum_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 7, 2), (7, 9, 2), (9, 5, 2), (22, 22, 4)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd13.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_final = rdd13.reduce(lambda x,y: (x[0]+ y[0], x[1] + y[1], x[2]+ y[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 43, 10)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = (rdd_final[0]/ rdd_final[2], rdd_final[1]/rdd_final[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, 4.3)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(input_path)\n",
    "rdd.collect()\n",
    "\n",
    "rdd1 = rdd.map(lambda a: a.split(\":\"))\n",
    "rdd1.collect()\n",
    "\n",
    "def find_labels(text):\n",
    "    l = []\n",
    "    words = []\n",
    "    words = text[1].split(\" \")\n",
    "    count = 1\n",
    "    for word in words:\n",
    "        l.append((word, (text[0], count)))\n",
    "        count += 1\n",
    "    return (l) \n",
    "\n",
    "rdd2 = rdd1.map(lambda a: find_labels(a))\n",
    "rdd2.collect()\n",
    "\n",
    "rdd3 = rdd2.flatMap(lambda x: x).groupByKey().mapValues(lambda iter: list(iter))\n",
    "rdd3.collect()\n",
    "\n",
    "def map_labels(i):\n",
    "    d = {}\n",
    "    for item in i:\n",
    "        if item[0] in d:\n",
    "            d[item[0]].append(item[1])\n",
    "        else:\n",
    "            d[item[0]] = [item[1]]\n",
    "    return (d)      \n",
    "\n",
    "invert = rdd3.mapValues(lambda i: map_labels(i)).sortByKey()\n",
    "invert.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
